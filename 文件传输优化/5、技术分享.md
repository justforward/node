

## 第一张

今天我来说下关于瞬时下载的技术分享

这里我将从三个方面来说下：首先是现状，包括现在整个瞬时下载包含的服务以及架构，然后第二个方面说下，目前在这个服务的实际项目中存在的一些问题，最后基于现存的问题以及一些调研工作给出未来瞬时下载技术升级的规划。


# 第三张

首先说下背景，
从我们的业务场景中，主要牵扯到四个方面，首先用户通过浏览器将模型文件上传到存储，提交作业任务与k8s的服务进行交互，并生成文件同步任务，指明从同步的接受方，此时文件同步服务根据同步任务将进行存储和超算之间的文件同步。

而我们瞬时下载所关注的就是存储和超算之间文件同步。

根据部署的不同位置的存储类型，分为两种，一种是本地存储，一般是用户自己提供的存储，一种是云存储，比如之前使用的上海存储，以及我们之前搭建的济南存储和无锡存储。


# 第四张

这里罗列了瞬时下载整个运行所牵扯的所有服务，部署位置包括存储、超算、k8s。主要包括发起同步的文件同步任务，超算中提供文件系统操作接口的服务，以及做路由匹配和协议转换的envoy组件和做代理转发的nginx组件。


# 第五张

下面是整体架构的介绍

第一步文件同步服务需要获取，同步任务，数据请求流向是从存储的cloud_file_sync_runner 从此k8s file-sync_center得到文件同步任务，假设得到的是从文件上传任务，即从存储上传到超算里面，那么数据请求流向从cloud_box_filestorage 中得到文件信息，然后通过envoy组件和nginx组件将文件发送到超算中。

这个是个简单的架构，根据存储条件所在的网络环境不同，会有不同的组件调整。


# 第六张

这里的请求流程，主要为了体现底层的文件系统接口，我们这里对文件发起的请求都是通过网络文件系统接口，然后操作底层文件系统接口，最终完成文件A在云存储和超算之间的传输。

# 第七张

接下来介绍第二块内容，主要聚焦在实际使用上，遇到的一些问题。

首先第一点是代码的可维护性低，通过前面的架构介绍，可以看到整个同步任务中牵扯的架构组件太多，链路过长。比如envoy做一些路由匹配和协议转换等，这样导致定位问题的时候过于繁琐，比如上次格物定位altair作业不回传的问题排查过程中，据我了解 整体耗时到4天的时间，他们是将每个链路的日志都打印出来，最终是由于enoy组件对响应体的大小有所限制导致的，具体信息可参考以下的链接。

第二点是文件系统接口整体不清晰，表现为 在接口定义上，grpc和http混在一起使用，同时基于当前的文件系统接口，上层包装的的接口过多，造成阅读代码困难。

第三点是，代码层面存在过多的鉴权，服务之间的交互目前是基于证书的，但是现在代码中对文件系统接口上存在一些过多鉴权。

# 第八张

问题存在的第二个方面，同步场景服务覆盖不全面。在ABAQUS软件的某次计算行为，我们看到在云存储里面的同步结果与超算的结果文件在大小是一致的，但是文件的md5不一致。

通过分析结果文件，我们发现在当前的计算场景里面，某个时刻会对已落磁盘的文件内容进行修改，而当前的文件同步代码，在最后一次全量同步的时候，会使用增量同步的最后一次md5缓存，跳过前面部分已落磁盘文件的检查。

# 第九张

最后一个核心问题，文件同步速度较慢，这个是我在公司内网服务器上对文件同步进行测速，速度徘徊在5m/s 左右，

通过对当前文件同步代码分析，发现一些问题，针对大的文件，是做分片进行传输的，针对文件分块的压缩处理，是在内存中压缩之后，再进行传输，同时选择的分块大小是5m，没有很好的发挥gzip优势，并且针对不同的文件都是用gzip算法做统一压缩处理。


# 第十张

针对上诉存在的三类问题，我们定下的优化规划目标包括三个：第一个是提高代码的可维护性，第二个是扩大同步覆盖场景，第三就是提升文件的传输速度。

右边的图展示了程序数据在整体网络的流动过程，这里我们参考osi五层网络模型，主要聚焦在应用层和传输层以及网络层，分别说明一下未来我们可以做哪些事情。

# 第十一张

首先是在应用层的优化，该层的优化会将规划中三个目标都会包含其中，第一个是提高代码的可维护性，其优化方向是，重构底层接口，减少调用链路，具体方案如右图，第一步，重构文件系统接口，统一为http调用，减少上层接口包装，方便代码阅读。第二步 来减少代码中无用的鉴权，这里我们认为只要通过证书的验证，即为安全请求，不需要再进行额外的鉴权，第三步 是移除envoy组件，减少整个调用链路，同时减少envoy组件带来的一些限制，由nginx做路由匹配。

# 第十二张

在应用层做的第二点优化是扩大同步覆盖场景，优化方向是支持修改 新增场景。具体实时方案为，参照unix中rsync代码实现，重构文件同步代码，Rsync作为unix系统中成熟的远程文件同步工具，采用增量编码，增量编码可以在源文件发生改变之后，仅仅同步改变的数据，极大减少网络传输量，很适合我们刚才提到的ABAQUS计算场景。这里粘贴了发明rsync算法作者发表的论文，感兴趣的同学可以了解一下。

# 第十二张

在应用层做的第三点优化是提升文件传输速度，优化方向包含 局部优化和全局优化，

其中局部优化方案主要是对特定软件的结果文件生成行为进行优化，比如之前分析了starccm+结果文件，发现在某些计算场景中，straccm软件会生成结果文件的副本，我们可以在同步任务中将这类文件加入黑名单中。

全局优化方案是针对所有结果文件同步，比如我们可以充分发挥http流式传输的特点，做到边压缩边传输，
同时随着更多优秀的压缩算法崛起，可以选择压缩吞吐量更高的压缩算法去替换gzip，这里我尝试使用zstd算法替换gzip，在实际测试中表现效果确实好于gzip。关于zstd的具体内容可以参阅下面的链接。

同时我们还可以进行分块策略，根据不同的文件选择不同的分块大小。最后我们还可以通过提升MD5的计算速度来进行优化。


# 第十三张

下面介绍传输层的优化，对于该层可以做的优化目标就是对文件传输速度进行优化。

在传输层 我们只能是对协议进行优化，目前想到的方案包括两个方面，首先第一方面是基于UDP的可靠传输协议，目前程序中依赖的http协议是基于tcp的，对于tcp的可靠传输 和流量控制大家都清楚，但是为了实现这些特性，TCP协议会为每一次传输的数据流维护一些状态信息，比如socket，序列号 窗口大小等，同时还需要包含可靠链接的建立和释放机制。但是UDP为了提高数据传输的吞吐量，不需要负责负责的保证机制，它仅为应用程序的通信提供少量协议，几乎等于将网络层的IP协议提到的传输层中。 将可靠性的保证迁移至用户态实现，从图上可以看到 QUIC 协议层就实现了可靠的数据传输，拥塞控制，加密，多路数据流。




# 第十四张

刚才提到的基于UDP的可靠性传输是将内核网络协议栈部分功能提到用户态实现，这一部分我们可以大胆一些将整个内核态协议栈进行替换为用户态协议栈，减少内核态与用户态的切换次数。

Client发送数据给server，数据首先到达网卡，经过两步到达应用程序 1）将数据从网卡的内存copy到内核协议栈，内核协议栈对数据包进行解析； 2）应用程序通过调用recv函数，将数据从内核copy进用户空间，得到应用层的数据包。

这里可以将数据从网卡发送到内核态协议栈进行数据拦截，然后由用户态协议栈进行解析，再发送给应用程序。


# 第十五张


整个过程分为两个部分。第一部分是从网卡中获取原始数据，具体做法是可以通过netmap实现，netmap用于实现用户态和网卡之间数据包的高性能传输，直接将网卡中的数据映射到一块内存中，应用程序可以直接通过mmap操作相应内存的数据

第二部分是数据处理和传输，从client发送的数据，走到传输层先是在数据前面加一个TCP协议头，到了网络层再加上ip头，进入数据链路层中再加上一个以太网头，再进入物理层的网卡就是把他从数字信号转换成为模拟信号传输了。整个过程是层层加头的过程，我们从接受方网卡中得到的数字信号就是最底下一层，那么用户态协议栈只要对这每一层解析，最后将数据传输给应用程序即可。





# 补充知识



zstd 是Facebook开源的一个无损压缩算法，
