

## 第一张

今天我分享的主题是瞬时下载技术的现状和未来

这里我将从三个方面展开说下：首先是现状，包括现在整个瞬时下载包含的服务以及整体架构，然后第二个方面说下，目前在这个服务在实际项目中存在的一些问题，最后基于现存的问题以及一些技术调研工作给出瞬时下载技术升级的未来规划。


# 第三张

首先说下背景，
从我们的业务场景中，主要牵扯到四个方面，首先用户通过浏览器将模型文件上传到存储空间中，然后提交作业任务的与k8s的服务进行交互，并生成文件同步任务，指明同步文件的来源方与接受方，此时文件同步服务根据同步任务进行存储和超算之间的文件同步。

而我们瞬时下载所关注的就是存储和超算之间的文件同步这块。

根据存储部署的位置，存储类型分为两种，一种是本地存储，一般是部署在用户本地，一种是云存储，比如之前使用的上海存储，以及我们之前搭建的济南存储和无锡存储。


# 第四张

这里罗列了瞬时下载项目牵扯的所有服务，里面包含的服务有：部署在存储侧的文件同步服务，以及部署在存储和超算中的文件系统接口相关服务，同时还有做路由匹配和协议转换的envoy组件还有就是做代理转发的nginx中间件。


# 第五张

下面是整体架构的介绍

首先文件同步服务需要获取同步任务，数据请求流向是从存储的cloud_file_sync_runner发出，然后经过enovy，nginx组件 从k8s 的file-sync_center中得到，假设得到的是文件上传任务，即从存储上传到超算里面，那么数据请求流将从cloud_box_filestorage 中得到文件信息，然后再通过envoy，nginx组件与超算中的scadapt服务进行交互，最终将文件发送到超算中。

目前罗列的是个简单的架构，根据存储条件所在的网络环境不同，会进行相关组件的调整。


# 第六张

下面是文件上传的请求流程，这张图主要为了体现相关的文件系统接口，文件同步服务对文件发起的请求都是通过网络文件系统来操作本地文件系统，最终完成文件A在云存储和超算之间的远程传输。

# 第七张

接下来介绍第二块内容，主要聚焦在实际使用上，遇到的一些问题。

首先第一点是代码的可维护性低，通过前面的架构介绍，可以看到整个瞬时下载服务中牵扯的组件太多，这样会导致定位问题的时候较为麻烦，比如上次格物定位altair作业不回传的问题，据我了解 整体耗时到4天的时间，他们是将每个链路的日志都打印出来，最终定位到是由于enoy组件对响应体的大小有所限制导致的，具体信息可参考以下的链接。

第二点是文件系统接口整体不清晰，表现为 在接口定义上，grpc和http混在一起使用，同时基于当前的文件系统接口，上层包装的的接口过多，造成阅读代码困难。

第三点是，代码层面存在过多的鉴权，服务之间的交互是基于证书的，但是目前在代码中对文件系统接口上存在一些鉴权操作。比如某些接口只能是可读操作。

# 第八张

第二个方面，同步场景覆盖不全面。比如ABAQUS软件的某次计算行为，我们看到在云存储里面的同步文某个结果文件与超算的结果文件在大小是一致的，但是文件的md5不一致。

通过分析结果文件，我们发现在当前的计算场景里面，该软件会在某个时刻对已落磁盘的文件内容进行修改，而当前的文件同步服务，在最后一次全量同步的时候，会使用增量同步的最后一次md5缓存，从而跳过前面已落磁盘部分的文件检查。

# 第九张

最后一个核心问题，文件同步的速度较慢，这个是我在公司内网服务器上对文件上传进行测速，速度一直徘徊在4m/s 左右，抛开一些硬件方面的限制问题，目前看到的传输速度是不理想的。

对当前文件同步代码进行分析后，目前发现了一些问题，针对文件分片的压缩处理，目前代码中是在内存中压缩完成之后，再进行传输，同时选择的分块大小是5m，没有很好的发挥gzip压缩算法的优势，并且针对不同的文件都是统一由gzip算法进行压缩处理。


# 第十张

针对上诉存在的三类问题，这里定下优化规划目标包括三个：第一个是提高代码的可维护性，第二个是扩大同步覆盖场景，第三就是提升文件的传输速度。

瞬时下载中的文件同步服务是基于网络的远程传输，右边的图展示了数据在整哥网络的流动过程，这里我们参考osi五层网络模型，主要聚焦在应用层、传输层以及网络层上，分别说明一下在这三层里面未来我们可以做哪些事情。

# 第十一张

首先是在应用层的优化，该层的优化会将涵盖规划中三个目标，第一个目标是提高代码的可维护性，其优化方向是，重构底层接口，减少调用链路，具体方案如下，第一步，重构文件系统接口，统一为http调用，减少上层接口包装，从而方便代码阅读。第二步是来减少代码中无用的鉴权，这里我们认为只要通过证书的验证，即为安全请求，不需要再进行额外的鉴权，第三步 是移除envoy组件，减少整个调用链路，同时减少envoy组件带来的一些限制，由nginx做路由匹配。

# 第十二张

在应用层做的第二点优化是扩大同步覆盖场景，优化方向是不仅仅支持结果文件的append-only生成行为，还要支持落入磁盘文件的修改、新增场景。具体实时方案为，参照unix中rsync的代码实现，重构当前文件同步代码，Rsync作为unix系统中成熟的远程文件同步工具，其特点是采用增量编码，增量编码可以在源文件发生改变之后，仅仅同步改变的数据，极大减少网络传输量，很适合我们刚才提到的ABAQUS计算场景。这里粘贴了发明rsync算法作者发表的论文，感兴趣的同学可以了解一下。

# 第十二张

在应用层做的第三点优化是提升文件传输速度，优化方向包含 局部优化和全局优化，

其中局部优化方案主要是对特定软件的结果文件生成行为进行优化，比如之前分析了starccm+结果文件的生成过程，发现在某些计算场景中，straccm软件会生成结果文件的副本，对这类文件可以不进行同步，目前我们可以在同步任务中将这类文件加入到黑名单中。

全局优化方案是针对所有结果文件，比如我们可以充分发挥http流式传输的特点，进行边压缩边传输，
同时随着更多优秀的压缩算法崛起，可以选择压缩吞吐量更高的压缩算法去替换gzip，这里我尝试使用zstd压缩算法替换gzip，在实际测试中表现效果确实好于gzip。关于zstd的具体内容可以参阅下面的链接。

同时我们还可以进行分块策略，根据不同的结果文件选择合适的分块大小。最后我们还可以通过提升MD5的计算速度来进行优化。


# 第十三张

下面介绍传输层的优化，对于该层可做的优化目标就是提升文件的传输速度。

在传输层 的方向是对协议进行优化，目前想到的方案包括两个方面，首先第一方面是基于UDP的可靠传输协议，目前程序中依赖的http协议是基于tcp的，对于tcp的可靠传输 和流量控制大家都清楚，但是为了实现这些特性，TCP协议会为每一次传输的数据流维护一些状态信息，比如socket，序列号 窗口大小等，同时还需要包含可靠链接的建立和释放机制。但是UDP为了提高数据传输的吞吐量，不需要负责的保证机制，它仅为应用程序的通信提供少量协议，几乎等于将网络层的IP协议提到的传输层中。 这里也有一些成熟的实现协议，比如用于http/2的QUIC，他是将可靠性的保证迁移至用户态实现。

第二个方面是在保证可靠传输里面选择表现更加优秀的拥塞控制算法，比如bbr算法




# 第十四张

刚才提到的基于UDP的可靠性传输是将内核网络协议栈部分功能提到用户态实现，这一部分我们可以再大胆一些将整个内核态协议栈替换为用户态协议栈，从而很大程度减少内核态与用户态的切换次数。

Client发送数据给server的过程中，数据首先到达网卡，经过两步到达应用程序 1）将数据从网卡的内存copy到内核协议栈，内核协议栈对数据包进行解析； 2）应用程序通过调用recv函数，将数据从内核copy进用户空间，从而得到应用层的数据包。

这里可以将数据从网卡发送到内核态协议栈进行数据拦截，然后由用户态协议栈进行解析，再发送给应用程序。


# 第十五张


整个过程分为两个部分。第一部分是从网卡中获取原始数据，具体做法是可以通过netmap实现，netmap用于实现用户态和网卡之间数据包的高性能传输，直接将网卡中的数据映射到一块内存中，应用程序可以直接通过mmap操作相应内存的数据

第二部分是数据处理和传输，从client发送的数据，走到传输层先是在数据前面加一个TCP协议头，到了网络层再加上ip头，进入数据链路层中再加上一个以太网头，再进入物理层的网卡就是把他从数字信号转换成为模拟信号传输了。整个过程是层层加头的过程，我们从接受方网卡中得到的数字信号就是最底下一层，那么用户态协议栈只要对这每一层解析，最后将数据传输给应用程序即可。





# 补充知识



zstd 是Facebook开源的一个无损压缩算法，
