


# 文件分块存储

## inode|block 概念

![[Pasted image 20230117095403.png]]


这个inode有文件元数据和Block数组（长度是15），数组中前两项指向Block 3和Block 11，表示数据在这两个块中存着。一共15个数组，每个对应的blocks大小为4k。一共存储的文件大小为15*4k=60k。不够实际中的使用。



把这 15 个槽位分作 4 个不同类别来用：

1.  前 12 个槽位（也就是 0 - 11 ）我们成为直接索引；
    
2.  第 13 个位置，我们称为 1 级索引；
    
3.  第 14 个位置，我们称为 2 级索引；
    
4.  第 15 个位置，我们称为 3 级索引；


### 直接索引

直接索引：能存 12 个 block 编号，每个 block 4K，就是 48K，也就是说，48K 以内的文件，前 12 个槽位存储编号就能完全 hold 住。

### 一级索引

一级索引：也就是说这里存储的编号指向的 block 里面存储的也是 block 编号，里面的编号指向用户数据。一个 block 4K，每个元素 4 字节，也就是有 1024 个编号位置可以存储。

这里存储的编号指向的block里面存储的也是block编号，里面的编号指向用户数。一级索引可寻址1024个block。每个block指向的文件大小为4k，也就是一共能存储4m大小的文件。

![[Pasted image 20230117095923.png]]

### 二级索引

二级索引是在一级索引的基础上，在加上一段，可寻址1024*1024个block。1024*1024*4k=4G 的空间。

![[Pasted image 20230117100307.png]]


### 三级索引

三级索引是在二级索引的基础上又多了一级，也就是说，有了 4G （1024*1024*1024）的空间来存储用户数据的 block 编号。所以二级索引能寻址 4T 的空间。

![[Pasted image 20230117100409.png]]


所以，在这种文件系统（如ext2）上，通过这种间接块索引的方式，最大能支撑的文件大小 = 48K + 4M + 4G + 4T ，约等于 4 T。


**这种多级索引寻址性能表现怎么样？**

在不超过 12 个数据块的小文件的寻址是最快的，访问文件中的任意数据理论只需要两次读盘，一次读 inode，一次读数据块。

访问大文件中的数据则需要最多五次读盘操作：inode、一级间接寻址块、二级间接寻址块、三级间接寻址块、数据块。


# 问题 cp 100G文件use time 0.2


首先通过ls命令查看的文件大小确实100G（逻辑大小）

```
sh-4.4# ls -lh  
-rw-r--r-- 1 root root 100G Mar  6 12:22 test.txt
```

但是copy起来为什么会这么快呢？

```
sh-4.4# time cp ./test.txt ./test.txt.cp  
real 0m0.107s  
user 0m0.008s  
sys 0m0.085s
```

一个 SATA 机械盘的写能力能到 150 M/s （大部分的机械盘都是到不了这个值的）就算非常不错了，正常情况下，copy 一个 100G 的文件至少要 682 秒 ( 100 G/ 150 M/s )，也就是 11 分钟。

实际情况却是 cp 一秒没到就完成了工作，惊呆了，为啥呢？

更诡异的是：他的文件系统只有 40 G，为啥里面会有一个 100 G的文件呢？

同事把我找来，看看这个诡异的问题。

## 分析文件

查看文件在磁盘上真实的大小（物理大小）

```
sh-4.4# du -sh ./test.txt  
2.0M ./test.txt
```

再看stat文件的基本信息：

```
sh-4.4# stat ./test.txt  
  File: ./test.txt  
  Size: 107374182400 Blocks: 4096       IO Block: 4096   regular file  
Device: 78h/120d Inode: 3148347     Links: 1  
Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)  
Access: 2021-03-13 12:22:00.888871000 +0000  
Modify: 2021-03-13 12:22:46.562243000 +0000  
Change: 2021-03-13 12:22:46.562243000 +0000  
 Birth: -

```

stat 命令输出解释：
1.  Size 为 107374182400（知识点：单位是字节），也就是 100G ；
2.  Blocks 这个指标显示为 4096（知识点：一个 Block 的单位固定是 512 字节，也就是一个扇区的大小），这里表示为 2M；
    

**划重点**：
-   Size 表示的是文件大小，这个也是大多数人看到的大小；
-   Blocks 表示的是物理实际占用空间；


## 为什么cp的那么快？

文件很大，但是真正的数据只有8K：所以在[0,4K]这位置有4K的数据，在[1T , 1T+4K] 处也有4K数据。

中间没有数据，这样的文件该如何写入硬盘？

1.  创建一个文件，这个时候分配一个 inode；
    
2.  在 [ 0，4K ] 的位置写入 4K 数据，这个时候只需要 一个 block，把这个编号写到 block[0] 这个位置保存起来；
    
3.  在 [ 1T，1T+4K ] 的位置写入 4K 数据，这个时候需要分配一个 block，因为这个位置已经落到三级索引才能表现的空间了，所以需要还需要分配出 3 个索引块；
    
4.  写入完成，close 文件；

实际上的存储图：

![[Pasted image 20230117101541.png]]

这个时候，我们的文件看起来是超大文件，size 等于 1T+4K ，但里面实际的数据只有 8 K，位置分别是 [ 0，4K ] ，[ 1T，1T+4K ]。


由于没写数据的地方不需要分配物理block块，所以实际上占用的物理空间只有8k

重点：文件 size 只是 inode 里面的一个属性，实际物理空间占用则是要看用户数据放了多少个 block ，没写数据的地方不用分配物理block块。

这样的文件其实就是稀疏文件， 它的逻辑大小和实际物理空间是不相等的。

所以当我们用cp命令去复制一个这样的文件时，那肯定迅速就完成了。

## 总结

好，我们再深入思考下，文件系统为什么能做到这一点？

1.  首先，最关键的是把磁盘空间切成离散的、定长的 block 来管理；
    
2.  然后，通过 inode 能查找到所有离散的数据（保存了所有的索引）；
    
3.  最后，实现索引块和数据块空间的后分配；
