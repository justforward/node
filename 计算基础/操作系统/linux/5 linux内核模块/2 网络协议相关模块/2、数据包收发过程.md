

# linux 网络--数据包的接受过程


## 网卡到内存

网卡需要有驱动才能工作，驱动是加载到内核中的模块，负责衔接网卡和内核的网络模块。
驱动在加载的时候需要将自己注册到网络模块，当相应的网卡收到数据包的时候，网络模块会调用相关的驱动程序去处理数据。

1、数据包从外面的网络进入到物理网卡，如果目的地不是该网卡并且该网卡，
2、网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化。
3、网卡通过硬件中断通知CPU，有数据来了
。。。。
4、驱动程序会读取网卡写到内存中的数据包，并且转化成内核网络模块能识别的格式。
。。。。
5、调用协议栈相关的函数，将数据包交给协议栈处理。


## 内核的网络模块

网络模块包含驱动？ 

Linux内核网络设备驱动程序位于数据链路层，网络设备主要由`net_device`结构组织的，设备的参数决定数据包是否需要分段。`net_device`结构包含了如下设备参数（部分）：


软中断会触发内核网络模块中的软中断处理函数，后续流程如下：

-   **7：** 内核中的ksoftirqd进程专门负责软中断的处理，当它收到软中断后，就会调用相应软中断所对应的处理函数，对于上面第6步中是网卡驱动模块抛出的软中断，ksoftirqd会调用网络模块的net_rx_action函数
-   **8：** net_rx_action调用网卡驱动里的poll函数来一个一个的处理数据包
-   **9：** 在pool函数中，驱动会一个接一个的读取网卡写到内存中的数据包，内存中数据包的格式只有驱动知道
-   **10：** 驱动程序将内存中的数据包转换成内核网络模块能识别的skb格式，然后调用napi_gro_receive函数
-   **11：** napi_gro_receive会处理[GRO](https://link.segmentfault.com/?enc=PIgDTxoHf13AvpH0vD9bjA%3D%3D.Fjh4HAafs8a%2BU%2Bs%2Bqngs9yWef4HacnyFtzRTtgMOd15m7gT1%2BD9Oc69Z1kBypohl)相关的内容，也就是将可以合并的数据包进行合并，这样就只需要调用一次协议栈。然后判断是否开启了[RPS](https://link.segmentfault.com/?enc=%2FZMUqxT8RIBrDXGGTfVDrw%3D%3D.nB7N3USvSWdNMiUd%2FkjGNV1X0nUC73W42jtGf0u5mEYMBhrG1kHf04%2Frzzg7mb4%2FvmV1alxmquLk1vuErzyyTGMok9ezyt185YersJ%2F3V9DW9gCwVuxt0Db%2F8nIOPvi0)，如果开启了，将会调用enqueue_to_backlog
-   **12：** 在enqueue_to_backlog函数中，会将数据包放入CPU的softnet_data结构体的input_pkt_queue中，然后返回，如果input_pkt_queue满了的话，该数据包将会被丢弃，queue的大小可以通过net.core.netdev_max_backlog来配置
-   **13：** CPU会接着在自己的软中断上下文中处理自己input_pkt_queue里的网络数据（调用__netif_receive_skb_core）
-   **14：** 如果没开启[RPS](https://link.segmentfault.com/?enc=Uqi0rgcNBIWc%2B7RXuYf5RA%3D%3D.aZ2RI9u3Ih0b6T8%2FTMBIR5syR5QXHy1i%2FnscM5XUTbyyGOVw64fJCJtuc0zxfAo4gojo%2BXAhR5%2F9s624tx4sv%2BudyBHHqdzraeY6InwsKoKQQBxI8UvbQDpJ7z828Cwf)，napi_gro_receive会直接调用__netif_receive_skb_core
-   **15：** 看是不是有AF_PACKET类型的socket（也就是我们常说的原始套接字），如果有的话，拷贝一份数据给它。tcpdump抓包就是抓的这里的包。
-   **16：** 调用协议栈相应的函数，将数据包交给协议栈处理。
-   **17：** 待内存中的所有数据包被处理完成后（即poll函数执行完成），启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知CPU

数据链路层：原始套接字（tcpdump抓包就是抓这里的包，）




## 协议栈

##### IP层 （网络层）

作用：

针对UDP包，所以第一步会进入IP层（网络层），然后一级一级的函数往下调

```
          |
          |
          ↓         promiscuous mode &&
      +--------+    PACKET_OTHERHOST (set by driver)   +-----------------+
      | ip_rcv |-------------------------------------->| drop this packet|
      +--------+                                       +-----------------+
          |
          |
          ↓
+---------------------+
| NF_INET_PRE_ROUTING |
+---------------------+
          |
          |
          ↓
      +---------+
      |         | enabled ip forword  +------------+        +----------------+
      | routing |-------------------->| ip_forward |------->| NF_INET_FORWARD |
      |         |                     +------------+        +----------------+
      +---------+                                                   |
          |                                                         |
          | destination IP is local                                 ↓
          ↓                                                 +---------------+
 +------------------+                                       | dst_output_sk |
 | ip_local_deliver |                                       +---------------+
 +------------------+
          |
          |
          ↓
 +------------------+
 | NF_INET_LOCAL_IN |
 +------------------+
          |
          |
          ↓
    +-----------+
    | UDP layer |
    +-----------+
```

-   **ip_rcv：** ip_rcv函数是IP模块的入口函数，在该函数里面，第一件事就是将垃圾数据包（目的mac地址不是当前网卡，但由于网卡设置了混杂模式而被接收进来）直接丢掉，然后调用注册在NF_INET_PRE_ROUTING上的函数
-   **NF_INET_PRE_ROUTING：** netfilter放在协议栈中的钩子，可以通过iptables来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走
-   **routing：** 进行路由，如果是目的IP不是本地IP，且没有开启ip forward功能，那么数据包将被丢弃，如果开启了ip forward功能，那将进入ip_forward函数
-   **ip_forward：** ip_forward会先调用netfilter注册的NF_INET_FORWARD相关函数，如果数据包没有被丢弃，那么将继续往后调用dst_output_sk函数
-   **dst_output_sk：** 该函数会调用IP层的相应函数将该数据包发送出去，同下一篇要介绍的数据包发送流程的后半部分一样。 
-   **ip_local_deliver**：如果上面**routing**的时候发现目的IP是本地IP，那么将会调用该函数，在该函数中，会先调用NF_INET_LOCAL_IN相关的钩子程序，如果通过，数据包将会向下发送到UDP层


总结：网络层里面进行数据包的过滤（iptables）

解包的过程：将IP协议头解开

##### UDP层

```
          |
          |
          ↓
      +---------+            +-----------------------+
      | udp_rcv |----------->| __udp4_lib_lookup_skb |
      +---------+            +-----------------------+
          |
          |
          ↓
 +--------------------+      +-----------+
 | sock_queue_rcv_skb |----->| sk_filter |
 +--------------------+      +-----------+
          |
          |
          ↓
 +------------------+
 | __skb_queue_tail |
 +------------------+
          |
          |
          ↓
  +---------------+
  | sk_data_ready |
  +---------------+

```

-   **udp_rcv：** udp_rcv函数是UDP模块的入口函数，它里面会调用其它的函数，主要是做一些必要的检查，其中一个重要的调用是__udp4_lib_lookup_skb，该函数会根据目的IP和端口找对应的socket，如果没有找到相应的socket，那么该数据包将会被丢弃，否则继续。
- **sock_queue_rcv_skb：** 主要干了两件事，一是**检查这个socket的receive buffer是不是满了，如果满了的话，丢弃该数据包**，然后就是调用sk_filter看这个包是否是满足条件的包，**如果当前socket上设置了[filter](https://link.segmentfault.com/?enc=baMsg5F4M%2BE8cijtN1tSnQ%3D%3D.ivrZvU0oLm1n59bz%2F%2B%2BVCmbW67j8L%2FqgUoh7Ovj3ZHu0uKvzPsa0wDwIpSAUWosfkHwGJnF4n8G%2B2YjtVcdS%2BQ%3D%3D)，且该包不满足条件的话，这个数据包也将被丢弃**（在Linux里面，每个socket上都可以像tcpdump（抓包工具）里面一样定义[filter](https://link.segmentfault.com/?enc=168J0TsZuO7jxQTzAg5ycw%3D%3D.1jvREnLGLYRMHgIA6bGndTkYDWNZ9JlXaoYQ9pQTLwcwmoQVZx3TNlGHxCb51JRDZvkCtDutOPioKAWrmz%2F1Ew%3D%3D)，不满足条件的数据包将会被丢弃）
-   **__skb_queue_tail：** 将数据包放入socket接收队列的末尾
-  **sk_data_ready：** 通知socket数据包已经准备好

调用完sk_data_ready之后，一个数据包处理完成，等待应用层程序来读取，上面所有函数的执行过程都在软中断的上下文中。


#### socket

应用层中存在的接受数据方式，存在两种：

1、recvfrom函数阻塞在那里等着数据来，这种情况下当socket收到通知后，recvfrom就会被唤醒，然后读取接收队列的数据。

2、通过epoll或者select监听相应的socket，当收到通知后，再调用recvfrom函数去读取接收队列的数据。



## 总结

了解数据包的接收流程有助于帮助我们搞清楚我们可以在哪些地方监控和修改数据包，哪些情况下数据包可能被丢弃，为我们处理网络问题提供了一些参考，同时了解netfilter中相应钩子的位置，对于了解iptables的用法有一定的帮助，同时也会帮助我们后续更好的理解Linux下的网络虚拟设备。



# linux 网络---数据包的发送过程

## socket 层

```
               +-------------+
               | Application |
               +-------------+
                     |
                     |
                     ↓
+------------------------------------------+
| socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP) |
+------------------------------------------+
                     |
                     |
                     ↓
           +-------------------+
           | sendto(sock, ...) |
           +-------------------+
                     |
                     |
                     ↓
              +--------------+
              | inet_sendmsg |
              +--------------+
                     |
                     |
                     ↓
             +---------------+
             | inet_autobind |
             +---------------+
                     |
                     |
                     ↓
               +-----------+
               | UDP layer |
               +-----------+

```


-   **socket(...)：** 创建一个socket结构体，并初始化相应的操作函数，由于我们定义的是UDP的socket，所以里面存放的都是跟UDP相关的函数
    
-   **sendto(sock, ...)：** 应用层的程序（Application）调用该函数开始发送数据包，该函数数会调用后面的inet_sendmsg
    
-   **inet_sendmsg：** 该函数主要是检查当前socket有没有绑定源端口，如果没有的话，调用inet_autobind分配一个，然后调用UDP层的函数
    
-   **inet_autobind：** 该函数会调用socket上绑定的get_port函数获取一个可用的端口，由于该socket是UDP的socket，所以get_port函数会调到UDP代码里面的相应函数。









# linux 虚拟设备tun/tap


linux内核中有一个网络设备管理层，处于网络设备驱动和协议栈之间，负责衔接他们之间的数据交互。驱动不需要了解协议栈的细节，协议栈也不需要了解设备驱动的细节。

对于一个网络设备来说，就像一个管道（pipe）一样，有两端，从其中任意一端收到的数据将从另一端发送出去。


比如一个物理网卡eth0，它的两端分别是内核协议栈（通过内核网络设备管理模块间接的通信）和外面的物理网络，从物理网络收到的数据，会转发给内核协议栈，而应用程序从协议栈发过来的数据将会通过物理网络发送出去。

那么对于一个虚拟网络设备呢？首先它也归内核的网络设备管理子系统管理，对于Linux内核网络设备管理模块来说，虚拟设备和物理设备没有区别，都是网络设备，都能配置IP，从网络设备来的数据，都会转发给协议栈，协议栈过来的数据，也会交由网络设备发送出去，至于是怎么发送出去的，发到哪里去，那是设备驱动的事情，跟Linux内核就没关系了，所以说虚拟网络设备的一端也是协议栈，而另一端是什么取决于虚拟网络设备的驱动实现。



上图中有两个应用程序A和B，都在用户层，而其它的socket、协议栈（Newwork Protocol Stack）和网络设备（eth0和tun0 虚拟网卡？）部分都在内核层，其实socket是协议栈的一部分，这里分开来的目的是为了看的更直观。

tun0是一个Tun/Tap虚拟设备，从上图中可以看出它和物理设备eth0的差别，它们的一端虽然都连着协议栈，但另一端不一样，eth0的另一端是物理网络，这个物理网络可能就是一个交换机，而tun0的另一端是一个用户层的程序，协议栈发给tun0的数据包能被这个应用程序读取到，并且应用程序能直接向tun0写数据。

从上面介绍过的流程可以看出来，tun/tap设备的用处是将协议栈中的部分数据包转发给用户空间的应用程序，给用户空间的程序一个处理数据包的机会。于是比较常用的数据压缩，加密等功能就可以在应用程序B里面做进去，tun/tap设备最常用的场景是VPN，包括tunnel以及应用层的IPSec等，比较有名的项目是[VTun](https://link.segmentfault.com/?enc=n%2BLLZeQtnJN3UNHT%2BkiOQw%3D%3D.Ee%2Bkpw%2FUVO7aUG6YrlXL8DywcLEvahPKvDrAzashlu8%3D)，有兴趣可以去了解一下。


tun和tap的区别

用户层程序通过tun设备只能读写IP数据包，而通过tap设备能读写链路层数据包，类似于普通socket和raw socket的差别一样，处理数据包的格式不一样。





网络设备管理层包含什么？
驱动和网络层之间构建一个网络接口核心层








# Linux操作系统原理—内核网络协议栈

https://cloud.tencent.com/developer/article/1869908


网络模块: 是否包含网络协议栈？
http://kerneltravel.net/blog/2020/network_ljr_no1/
